{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "257629ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import chess\n",
    "import chess.engine\n",
    "from chess.engine import Cp\n",
    "import helperfuncs\n",
    "import bitboards\n",
    "import net\n",
    "import autoencoder\n",
    "import alphabeta\n",
    "import mctsAZ\n",
    "import mcts_custom\n",
    "\n",
    "MAX_MOVES = 64 ##in halfmoves\n",
    "DRAW_CUTOFF = 0 ##in centipawns\n",
    "param = 0.2 ##number in range (0, 1) used in PARAM 'ReinforcementType (simple linear combination of TD and Monte-Carlo learning)'\n",
    "engine = chess.engine.SimpleEngine.popen_uci(\"/usr/games/stockfish\")\n",
    "\n",
    "SearchType = Enum('SearchType', 'MINIMAX MCTS CUSTOM')\n",
    "ReinforcementType = Enum('ReinforcementType', 'MC TD PARAM')\n",
    "winner_to_num = {chess.WHITE: 1, chess.BLACK: 0, None: 0.5}\n",
    "\n",
    "class Encode(object):\n",
    "    def __init__(self, encoder):\n",
    "        self.encoder = encoder\n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        return self.encoder.encode(sample)\n",
    "    \n",
    "class SearchDataset(Dataset):\n",
    "    def __init__(self, size, transform, reinf, *args):\n",
    "        self.data = get_dataset(size, reinf, *args)\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return [self.transform(self.data[idx][0])] + self.data[idx][1:]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "def get_dataset(size, reinf, *args):\n",
    "    '''Get dataset for NN training no smaller than specified \"size\".\n",
    "    Args are the \"generate_game\" function parameters.'''\n",
    "    dataset = []\n",
    "    \n",
    "    while len(dataset) < size:\n",
    "        game = generate_game(*args)\n",
    "        \n",
    "        winner, state =  -1, game[-1].state\n",
    "\n",
    "        i = 0\n",
    "        while state.can_claim_draw():\n",
    "            state = game[- (1 + i)].state\n",
    "            i += 1\n",
    "            \n",
    "        outcome = state.outcome()\n",
    "        \n",
    "        score = engine.analyse(state, chess.engine.Limit(time=1))[\"score\"].white()\n",
    "        if score > Cp(DRAW_CUTOFF):\n",
    "            winner = chess.WHITE\n",
    "        elif score < Cp(DRAW_CUTOFF):\n",
    "            winner = chess.BLACK\n",
    "        else:\n",
    "            winner = None\n",
    "                        \n",
    "        for nd in game:\n",
    "            val = get_value(reinf, args[3], winner, nd)\n",
    "            \n",
    "            if args[3] == SearchType.MINIMAX:\n",
    "                position = bitboards.bitboard_to_cnn_input(bitboards.bitboard(nd.get_node().state)).unsqueeze(0).cuda()\n",
    "                dataset.append([position, val])\n",
    "                \n",
    "            elif args[3] == SearchType.MCTS:\n",
    "                position = bitboards.bitboard_to_cnn_input(bitboards.bitboard(nd.state)).unsqueeze(0).cuda()\n",
    "                moves = [move.uci() for move in nd.moves]\n",
    "                policy = helperfuncs.policy_from_probability([[moves[i], child.actionValue] for i, child in enumerate(nd.childNodes)])\n",
    "                dataset.append([position, val, policy.cuda()])\n",
    "                \n",
    "            elif args[3] == SearchType.CUSTOM:\n",
    "                position = bitboards.bitboard_to_cnn_input(bitboards.bitboard(nd.state)).unsqueeze(0).cuda()\n",
    "                moves = [move.uci() for move in nd.moves]\n",
    "                choiceProbability = nd.choiceProbability\n",
    "                policy = helperfuncs.policy_from_probability([[moves[i], choiceProbability.value(i)] for i in range(len(choiceProbability.x))])\n",
    "                dataset.append([position, val, policy.cuda()])\n",
    "            \n",
    "    return dataset\n",
    "        \n",
    "def generate_game(board, nnet, encoder, search_tree, *args):\n",
    "    '''Generate the chess game given the starting \"board\" position. Args depend on chosen search type. \n",
    "    Three parameters for MINIMAX: depth, lower bound and higher bound of aspiration window.\n",
    "    One parameter for MCTS and CUSTOM: number of rollouts.'''\n",
    "    \n",
    "    game, moves = [], 0\n",
    "    \n",
    "    while not stop_cond(board, moves):\n",
    "        \n",
    "        if search_tree == SearchType.MINIMAX:\n",
    "            node = alphabeta.alphabeta(alphabeta.Node(board), args[0], args[1], args[2], nnet, encoder)\n",
    "            board = node.get_node().state\n",
    "            \n",
    "        elif search_tree == SearchType.MCTS:\n",
    "            tree = mctsAZ.Mcts(board, nnet, encoder)\n",
    "            node = tree.search(args[0])\n",
    "            board = node.state\n",
    "            \n",
    "        elif search_tree == SearchType.CUSTOM:\n",
    "            tree = mcts_custom.Mcts(board, nnet, encoder)\n",
    "            node = tree.search(args[0])\n",
    "            board = node.state\n",
    "                    \n",
    "        game.append(node)\n",
    "        moves += 1\n",
    "\n",
    "    return game\n",
    "\n",
    "def stop_cond(board, moves):\n",
    "    end = False\n",
    "    \n",
    "    if board.is_checkmate() or board.is_stalemate() or board.is_insufficient_material():\n",
    "        end = True\n",
    "    elif moves > MAX_MOVES:\n",
    "        end = True\n",
    "        \n",
    "    return end\n",
    "\n",
    "def get_value(reinf_type, search_type, winner, node):\n",
    "    if reinf_type == ReinforcementType.MC:\n",
    "        val = winner_to_num[winner]\n",
    "        \n",
    "    elif reinf_type == ReinforcementType.TD or ReinforcementType.PARAM:\n",
    "        if search_type == SearchType.MINIMAX:\n",
    "            val = node.get_val()[0]\n",
    "        elif search_type == SearchType.MCTS or search_type == SearchType.CUSTOM:\n",
    "            val = node.actionValue\n",
    "            \n",
    "    elif reinf_type == ReinforcementType.PARAM:\n",
    "        val = param * val + (1 - param) * winner_to_num[winner]\n",
    "    \n",
    "    return torch.Tensor([val]).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27c4bf61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game result mean:  1.0  Standard deviation:  0.0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-ea19bfe3a718>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Game result mean: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatistics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" Standard deviation: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatistics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdev\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Example policy: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "##checking few basic statistics about generated datasets\n",
    "import statistics\n",
    "\n",
    "encoder = autoencoder.autoencoder().cuda()\n",
    "encoder.load_state_dict(torch.load(\"autoencoderftest2.pt\"))\n",
    "nnet = net.Net().cuda()\n",
    "nnet.load_state_dict(torch.load(\"nnet_mcts.pt\"))\n",
    "\n",
    "args = (chess.Board(), nnet, encoder, SearchType.MCTS, 5)\n",
    "dataset = SearchDataset(256, Encode(encoder), ReinforcementType.MC, *args)\n",
    "\n",
    "vals, policies, positions = [], [], []\n",
    "for position, val, policy in dataset:\n",
    "    vals.append(val.item())\n",
    "    policies.append(policy)\n",
    "    positions.append(position)\n",
    "    \n",
    "print(\"Game result mean: \", statistics.mean(vals), \" Standard deviation: \", statistics.stdev(vals))\n",
    "\n",
    "print(\"Example policy: \", policies[32][0].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f91e21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss:  22.346508514185643\n",
      "Training loss:  21.73310094980093\n",
      "\n",
      "Test loss:  63.11739533253205\n",
      "Training loss:  63.39016001774714\n",
      "\n",
      "Test loss:  37.04943624938912\n",
      "Training loss:  36.57748533550062\n",
      "\n",
      "Test loss:  30.48167440804447\n",
      "Training loss:  27.523625548456756\n",
      "\n",
      "Test loss:  33.338765064097664\n",
      "Training loss:  36.52845764160156\n",
      "\n",
      "Test loss:  34.075659840850676\n",
      "Training loss:  33.41646298495206\n",
      "\n",
      "Test loss:  33.29788592998798\n",
      "Training loss:  32.209083322378305\n",
      "\n",
      "Test loss:  31.617032596174052\n",
      "Training loss:  32.52634529445482\n",
      "\n",
      "Test loss:  29.50395218367429\n",
      "Training loss:  29.519256591796875\n",
      "\n",
      "Test loss:  28.621310392557486\n",
      "Training loss:  29.1707493878793\n",
      "\n",
      "Test loss:  29.406343429406633\n",
      "Training loss:  27.93659449986049\n",
      "\n",
      "Test loss:  30.884253887833683\n",
      "Training loss:  30.268284548245944\n",
      "\n",
      "Test loss:  26.679202706473216\n",
      "Training loss:  25.58304247174944\n",
      "\n",
      "Test loss:  25.99088401906741\n",
      "Training loss:  26.50043509988224\n",
      "\n",
      "Test loss:  27.38886017114186\n",
      "Training loss:  28.73720427120433\n",
      "\n",
      "Test loss:  24.772946537351146\n",
      "Training loss:  23.602961147532742\n",
      "\n",
      "Test loss:  26.444489561836676\n",
      "Training loss:  27.681289672851562\n",
      "\n",
      "Test loss:  28.381712877762475\n",
      "Training loss:  27.514866579495944\n",
      "\n",
      "Test loss:  28.19896533721229\n",
      "Training loss:  25.489971160888672\n",
      "\n",
      "Test loss:  24.046106911875718\n",
      "Training loss:  26.789568974421574\n",
      "\n",
      "Test loss:  28.4449443893739\n",
      "Training loss:  26.258093862822562\n",
      "\n",
      "Test loss:  26.4538347023955\n",
      "Training loss:  27.564380645751953\n",
      "\n",
      "Test loss:  27.834461107497635\n",
      "Training loss:  27.657181876046316\n",
      "\n",
      "Test loss:  24.114466494035206\n",
      "Training loss:  22.239592708758455\n",
      "\n",
      "Test loss:  22.097686707027375\n",
      "Training loss:  22.733652598822296\n",
      "\n",
      "Test loss:  27.582867538719846\n",
      "Training loss:  26.793344497680664\n",
      "\n",
      "Test loss:  21.64736406920386\n",
      "Training loss:  23.225685002253606\n",
      "\n",
      "Test loss:  23.550137475849485\n",
      "Training loss:  24.29402786938112\n",
      "\n",
      "Test loss:  24.52597794454606\n",
      "Training loss:  23.939876980251736\n",
      "\n",
      "Test loss:  25.139390977891953\n",
      "Training loss:  25.6180393334591\n",
      "\n",
      "Test loss:  25.69291309845753\n",
      "Training loss:  25.979393181434045\n",
      "\n",
      "Test loss:  26.749104328938312\n",
      "Training loss:  27.827746535792496\n",
      "\n",
      "Test loss:  25.866926566633598\n",
      "Training loss:  25.649319661458332\n",
      "\n",
      "Test loss:  30.242896891991997\n",
      "Training loss:  29.9187811683206\n",
      "\n",
      "Test loss:  25.925992529991113\n",
      "Training loss:  25.609776960100447\n",
      "\n",
      "Test loss:  27.28109753541272\n",
      "Training loss:  25.029292713512074\n",
      "\n",
      "Test loss:  24.052826417976593\n",
      "Training loss:  23.905221650094695\n",
      "\n",
      "Test loss:  30.82834912434277\n",
      "Training loss:  30.52623051234654\n",
      "\n",
      "Test loss:  27.277521483460614\n",
      "Training loss:  25.413729600503412\n",
      "\n",
      "Test loss:  25.403376495990248\n",
      "Training loss:  27.048408844891718\n",
      "\n",
      "Test loss:  28.066390203198356\n",
      "Training loss:  28.374818613831426\n",
      "\n",
      "Test loss:  22.154120603132476\n",
      "Training loss:  22.412976693415988\n",
      "\n",
      "Test loss:  23.939622173518508\n",
      "Training loss:  23.169285225145746\n",
      "\n",
      "Test loss:  22.886915621881236\n",
      "Training loss:  23.00320226495916\n",
      "\n",
      "Test loss:  23.78974581041465\n",
      "Training loss:  24.14714514690897\n",
      "\n",
      "Test loss:  26.656495268584766\n",
      "Training loss:  26.973587094820463\n",
      "\n",
      "Test loss:  27.17433938929285\n",
      "Training loss:  25.88114501953125\n",
      "\n",
      "Test loss:  25.561062900178847\n",
      "Training loss:  22.226077809053308\n",
      "\n",
      "Test loss:  25.643154219225035\n",
      "Training loss:  25.887784071371588\n",
      "\n",
      "Test loss:  21.24695497047244\n",
      "Training loss:  22.14520155493893\n",
      "\n",
      "Test loss:  23.1106100362341\n",
      "Training loss:  23.1237455255845\n",
      "\n",
      "Test loss:  29.72464288449755\n",
      "Training loss:  25.570351432351504\n",
      "\n",
      "Test loss:  24.685808091623347\n",
      "Training loss:  25.545945085797992\n",
      "\n",
      "Test loss:  23.06579323583001\n",
      "Training loss:  20.85138912477355\n",
      "\n",
      "Test loss:  22.25560544396415\n",
      "Training loss:  21.374564537635216\n",
      "\n",
      "Test loss:  26.397645075404228\n",
      "Training loss:  24.005884426743236\n",
      "\n",
      "Test loss:  21.200545053633434\n",
      "Training loss:  19.923003743489584\n",
      "\n",
      "Test loss:  21.56867658501327\n",
      "Training loss:  21.582440694173176\n",
      "\n",
      "Test loss:  21.637253736686905\n",
      "Training loss:  24.617258071899414\n",
      "\n",
      "Test loss:  26.243061043387378\n",
      "Training loss:  27.6891908084645\n",
      "\n",
      "Test loss:  23.988892154194698\n",
      "Training loss:  22.33400362188166\n",
      "\n",
      "Test loss:  23.199149376074775\n",
      "Training loss:  22.26023962838309\n",
      "\n",
      "Test loss:  22.555548527264836\n",
      "Training loss:  23.77580180312648\n",
      "\n",
      "Test loss:  28.066602087063565\n",
      "Training loss:  26.010558360331768\n",
      "\n",
      "Test loss:  24.41298662027464\n",
      "Training loss:  26.52435066330601\n",
      "\n",
      "Test loss:  29.77258686774333\n",
      "Training loss:  30.74866986630568\n",
      "\n",
      "Test loss:  23.984045948529634\n",
      "Training loss:  23.729341700457145\n",
      "\n",
      "Test loss:  24.489465430574388\n",
      "Training loss:  22.294146797873758\n",
      "\n",
      "Test loss:  21.032764348031034\n",
      "Training loss:  22.586423959305037\n",
      "\n",
      "Test loss:  24.654339839766898\n",
      "Training loss:  24.068572776905004\n",
      "\n",
      "Test loss:  28.272077182788944\n",
      "Training loss:  27.107700857249174\n",
      "\n",
      "Test loss:  24.14976780980199\n",
      "Training loss:  23.844366015809953\n",
      "\n",
      "Test loss:  24.653003098226392\n",
      "Training loss:  24.463349413515918\n",
      "\n",
      "Test loss:  26.7134164283683\n",
      "Training loss:  30.651730970902875\n",
      "\n",
      "Test loss:  21.961194188317588\n",
      "Training loss:  24.739396645989217\n",
      "\n",
      "Test loss:  25.5664300462746\n",
      "Training loss:  24.593554859430018\n",
      "\n",
      "Test loss:  28.604717598424212\n",
      "Training loss:  28.319461178469968\n",
      "\n",
      "Test loss:  23.673647041780402\n",
      "Training loss:  21.36842357751095\n",
      "\n",
      "Test loss:  25.738454773355734\n",
      "Training loss:  27.13860637119838\n",
      "\n",
      "Test loss:  26.150300198038696\n",
      "Training loss:  25.304325970736418\n",
      "\n",
      "Test loss:  25.47975721976057\n",
      "Training loss:  25.475642190050724\n",
      "\n",
      "Test loss:  26.954548676159916\n",
      "Training loss:  26.466609221238357\n",
      "\n",
      "Test loss:  26.303277160895703\n",
      "Training loss:  24.31734848022461\n",
      "\n",
      "Test loss:  26.609875744217184\n",
      "Training loss:  25.994705827268834\n",
      "\n",
      "Test loss:  24.609622245159905\n",
      "Training loss:  25.44016911433293\n",
      "\n",
      "Test loss:  29.32001745769479\n",
      "Training loss:  25.960531041242074\n",
      "\n",
      "Test loss:  30.097991644752934\n",
      "Training loss:  30.272064994363223\n",
      "\n",
      "Test loss:  22.717069596570735\n",
      "Training loss:  24.0181523836576\n",
      "\n",
      "Test loss:  24.863618990083832\n",
      "Training loss:  24.945430408824574\n",
      "\n",
      "Test loss:  26.851850253432545\n",
      "Training loss:  26.333113549460826\n",
      "\n",
      "Test loss:  25.955443417068814\n",
      "Training loss:  26.076203228676157\n",
      "\n",
      "Test loss:  27.87254388383824\n",
      "Training loss:  26.544404725770693\n",
      "\n",
      "Test loss:  24.62131411312548\n",
      "Training loss:  25.3516114455003\n",
      "\n",
      "Test loss:  23.455429578993055\n",
      "Training loss:  20.00800333658854\n",
      "\n",
      "Test loss:  22.714459670799364\n",
      "Training loss:  19.535852625750113\n",
      "\n",
      "Test loss:  23.632961303907507\n",
      "Training loss:  23.18630907012195\n",
      "\n",
      "Test loss:  24.680413440265486\n",
      "Training loss:  23.394852026184992\n",
      "\n",
      "Test loss:  24.286435279449954\n",
      "Training loss:  24.560196031842914\n",
      "\n",
      "Test loss:  22.555136017814018\n",
      "Training loss:  23.964125178410455\n",
      "\n",
      "Test loss:  22.01173217891494\n",
      "Training loss:  20.818866280948413\n",
      "\n",
      "Test loss:  22.803609726523003\n",
      "Training loss:  21.49010432850231\n",
      "\n",
      "Test loss:  22.785453359991372\n",
      "Training loss:  23.25139913985978\n",
      "\n",
      "Test loss:  22.89202801750997\n",
      "Training loss:  23.62604268391927\n",
      "\n",
      "Test loss:  24.247075576312245\n",
      "Training loss:  23.27126693725586\n",
      "\n",
      "Test loss:  21.375949014541277\n",
      "Training loss:  20.53643798828125\n",
      "\n",
      "Test loss:  23.921022074361428\n",
      "Training loss:  22.68221043828708\n",
      "\n",
      "Test loss:  23.442316874921563\n",
      "Training loss:  21.199326168407094\n",
      "\n",
      "Test loss:  27.262148252476607\n",
      "Training loss:  25.613440195719402\n",
      "\n",
      "Test loss:  25.694854461944306\n",
      "Training loss:  28.238113403320312\n",
      "\n",
      "Test loss:  23.317548984063123\n",
      "Training loss:  24.497835506092418\n",
      "\n",
      "Test loss:  26.067532701557184\n",
      "Training loss:  25.08269141702091\n",
      "\n",
      "Test loss:  26.58492702273591\n",
      "Training loss:  26.520644496863998\n",
      "\n",
      "Test loss:  26.886039495029063\n",
      "Training loss:  23.314353942871094\n",
      "\n",
      "Test loss:  22.10445382920696\n",
      "Training loss:  21.764839835788894\n",
      "\n",
      "Test loss:  24.258705166889005\n",
      "Training loss:  21.617576599121094\n",
      "\n",
      "Test loss:  22.619110230476625\n",
      "Training loss:  21.47734832763672\n",
      "\n",
      "Test loss:  25.106831268279503\n",
      "Training loss:  22.212911869929385\n",
      "\n",
      "Test loss:  22.93158201704201\n",
      "Training loss:  20.997260710772345\n",
      "\n",
      "Test loss:  24.036501012409143\n",
      "Training loss:  25.1372127532959\n",
      "\n",
      "Test loss:  24.841010043152377\n",
      "Training loss:  24.03200980944511\n",
      "\n",
      "Test loss:  21.28470534611733\n",
      "Training loss:  19.63805711125753\n",
      "\n",
      "Test loss:  25.971396377549567\n",
      "Training loss:  26.134457522875643\n",
      "\n",
      "Test loss:  27.536672652007493\n",
      "Training loss:  26.318537270150532\n",
      "\n",
      "Test loss:  23.367133413461538\n",
      "Training loss:  24.015511204646184\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss:  24.826965087401366\n",
      "Training loss:  25.730371822010387\n",
      "\n",
      "Test loss:  25.774936486258724\n",
      "Training loss:  26.224432373046874\n",
      "\n",
      "Test loss:  23.523481114316546\n",
      "Training loss:  23.354720189021183\n",
      "\n",
      "Test loss:  24.29465023738401\n",
      "Training loss:  26.273045708151425\n",
      "\n",
      "Test loss:  22.351040303764822\n",
      "Training loss:  24.003846112419577\n",
      "\n",
      "Test loss:  23.171485406242326\n",
      "Training loss:  23.47867766423012\n",
      "\n",
      "Test loss:  27.421334527765662\n",
      "Training loss:  27.288237179026883\n",
      "\n",
      "Test loss:  24.63427040929542\n",
      "Training loss:  23.981454737046185\n",
      "\n",
      "Test loss:  20.689387906932463\n",
      "Training loss:  20.056811346524004\n",
      "\n",
      "Test loss:  26.247182361319595\n",
      "Training loss:  25.63704480622944\n",
      "\n",
      "Test loss:  28.07070470169021\n",
      "Training loss:  28.9474316221295\n",
      "\n",
      "Test loss:  23.41047933430015\n",
      "Training loss:  20.902325370094992\n",
      "\n",
      "Test loss:  24.823327784087336\n",
      "Training loss:  26.471135515155215\n",
      "\n",
      "Test loss:  27.766242441803303\n",
      "Training loss:  25.456164640538834\n",
      "\n",
      "Test loss:  23.607043740915696\n",
      "Training loss:  22.54839206749285\n",
      "\n",
      "Test loss:  25.22778412702519\n",
      "Training loss:  24.688365391322545\n",
      "\n",
      "Test loss:  24.51582156910616\n",
      "Training loss:  24.397477467854817\n",
      "\n",
      "Test loss:  26.345919886997766\n",
      "Training loss:  24.741002546037947\n",
      "\n",
      "Test loss:  21.858657292482825\n",
      "Training loss:  22.358355425406195\n",
      "\n",
      "Test loss:  26.307455343304035\n",
      "Training loss:  26.754191230325137\n",
      "\n",
      "Test loss:  23.989254941608362\n",
      "Training loss:  23.255125427246092\n",
      "\n",
      "Test loss:  24.017445559389394\n",
      "Training loss:  23.257921703045184\n",
      "\n",
      "Test loss:  21.37508196507636\n",
      "Training loss:  21.500014173067534\n",
      "\n",
      "Test loss:  24.296443654949115\n",
      "Training loss:  22.861021677652996\n",
      "\n",
      "Test loss:  26.199035034179687\n",
      "Training loss:  27.752156459923945\n",
      "\n",
      "Test loss:  25.569175999077437\n",
      "Training loss:  27.564964616802378\n",
      "\n",
      "Test loss:  23.97951807123203\n",
      "Training loss:  25.326133109427786\n",
      "\n",
      "Test loss:  25.390377044677734\n",
      "Training loss:  23.710224151611328\n",
      "\n",
      "Test loss:  23.248829942438697\n",
      "Training loss:  21.869492025936353\n",
      "\n",
      "Test loss:  23.84230552187342\n",
      "Training loss:  23.429628718983043\n",
      "\n",
      "Test loss:  25.318382332054757\n",
      "Training loss:  25.439332326253254\n",
      "\n",
      "Test loss:  22.139372028052975\n",
      "Training loss:  22.909214564732142\n",
      "\n",
      "Test loss:  24.11688314679498\n",
      "Training loss:  24.321765796558278\n",
      "\n",
      "Test loss:  23.73786782954938\n",
      "Training loss:  26.02747980753581\n",
      "\n",
      "Test loss:  28.20822016644063\n",
      "Training loss:  29.39507824441661\n",
      "\n",
      "Test loss:  26.162142148497658\n",
      "Training loss:  28.5843267587515\n",
      "\n",
      "Test loss:  25.116348497096865\n",
      "Training loss:  25.602083957556523\n",
      "\n",
      "Test loss:  25.260768594057275\n",
      "Training loss:  25.356217569379663\n",
      "\n",
      "Test loss:  25.282171920939167\n",
      "Training loss:  25.837409075568704\n",
      "\n",
      "Test loss:  24.302505724357836\n",
      "Training loss:  23.378096444266184\n",
      "\n",
      "Test loss:  24.71664050874256\n",
      "Training loss:  24.74891117640904\n",
      "\n",
      "Test loss:  21.465566847881682\n",
      "Training loss:  20.811650668873508\n",
      "\n",
      "Test loss:  24.111960118510332\n",
      "Training loss:  22.858833085245163\n",
      "\n",
      "Test loss:  23.370760552502034\n",
      "Training loss:  25.22834111603213\n",
      "\n",
      "Test loss:  23.422405321608743\n",
      "Training loss:  22.71833883749472\n",
      "\n",
      "Test loss:  22.64597115001813\n",
      "Training loss:  21.93495478774562\n",
      "\n",
      "Test loss:  23.12457937593958\n",
      "Training loss:  25.867338231893687\n",
      "\n",
      "Test loss:  26.208433731742527\n",
      "Training loss:  24.993690604594217\n",
      "\n",
      "Test loss:  24.135133893302317\n",
      "Training loss:  25.55222420325646\n",
      "\n",
      "Test loss:  30.04016547946755\n",
      "Training loss:  25.29856957329644\n",
      "\n",
      "Test loss:  23.126094411013156\n",
      "Training loss:  20.650867405222424\n",
      "\n",
      "Test loss:  24.330537090151328\n",
      "Training loss:  23.68117284063083\n",
      "\n",
      "Test loss:  22.69634664864641\n",
      "Training loss:  22.778725132797703\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import bitboards\n",
    "import autoencoder\n",
    "import pgn_reader as reader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import math\n",
    "import gc\n",
    "\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "dsetSize, batch = 2**12, 2**8\n",
    "#autoEncoder = autoencoder.autoencoder().cuda()\n",
    "autoEncoder =  torch.load('autoencoder10.pt').cuda()\n",
    "criterion = nn.BCELoss(reduction='sum')\n",
    "optimizer = optim.Adam(autoEncoder.parameters())\n",
    "\n",
    "class chessDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_list):\n",
    "        self.data = data_list\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "            return self.data[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "data = reader.get_dataset(dsetSize)\n",
    "while data:\n",
    "    random.shuffle(data)\n",
    "    \n",
    "    dataset = chessDataset(data[:math.floor(1/16*len(data))])\n",
    "    testset = chessDataset(data[math.floor(1/16*len(data)):-1])\n",
    "\n",
    "    DataLoader = torch.utils.data.DataLoader(dataset, batch_size = batch, shuffle = True)\n",
    "    TestLoader = torch.utils.data.DataLoader(testset, batch_size = batch, shuffle = True)   \n",
    "    \n",
    "    autoEncoder.eval()\n",
    "    test_loss = 0\n",
    "    for x in TestLoader:\n",
    "        x = x.cuda()\n",
    "        y = autoEncoder(x).cuda()\n",
    "        loss = criterion(y, x)\n",
    "        test_loss += loss.item()\n",
    "    print(\"Test loss: \", test_loss / len(testset))\n",
    "        \n",
    "    autoEncoder.train()\n",
    "    training_loss = 0\n",
    "    for x in DataLoader:\n",
    "        optimizer.zero_grad()\n",
    "        x = x.cuda()\n",
    "        y = autoEncoder(x).cuda()\n",
    "        loss = criterion(y, x)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        training_loss += loss.item()\n",
    "    print(\"Training loss: \", training_loss / len(dataset), end='\\n\\n')\n",
    "    \n",
    "    data = reader.get_dataset(dsetSize)\n",
    "    \n",
    "    torch.save(autoEncoder, 'autoencoder11.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

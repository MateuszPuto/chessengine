{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "257629ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import chess\n",
    "import chess.engine\n",
    "from chess.engine import Cp\n",
    "import helperfuncs\n",
    "import bitboards\n",
    "import net\n",
    "import autoencoder\n",
    "import alphabeta\n",
    "import mctsAZ\n",
    "import mcts_custom\n",
    "\n",
    "engine = chess.engine.SimpleEngine.popen_uci(\"/usr/games/stockfish\")\n",
    "SearchType = Enum('SearchType', 'MINIMAX MCTS CUSTOM')\n",
    "ReinforcementType = Enum('ReinforcementType', 'MC TD PARAM')\n",
    "winner_to_num = {chess.WHITE: 1, chess.BLACK: 0, None: 0.5}\n",
    "\n",
    "class Encode(object):\n",
    "    def __init__(self, encoder):\n",
    "        self.encoder = encoder\n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        return self.encoder.encode(sample)\n",
    "    \n",
    "class SearchDataset(Dataset):\n",
    "    def __init__(self, size, transform, reinf, game_generator, *args):\n",
    "        self.data = game_generator.get_dataset(size, reinf, *args)\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return [self.transform(self.data[idx][0])] + self.data[idx][1:]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        \n",
    "class GameGenerator:\n",
    "    def __init__(self, max_moves, draw_cutoff, param):\n",
    "        ## MAX_MOVES in halfmoves\n",
    "        ## DRAW_CUTOFF in centipawns\n",
    "        ## PARAM number in range (0, 1) used in PARAM 'ReinforcementType (simple linear combination of TD and Monte-Carlo learning)'\n",
    "        \n",
    "        self.MAX_MOVES = max_moves\n",
    "        self.DRAW_CUTOFF = draw_cutoff\n",
    "        self.PARAM = param\n",
    "\n",
    "    def get_dataset(self, size, reinf, *args):\n",
    "        '''Get dataset for NN training no smaller than specified \"size\".\n",
    "        Args are the \"generate_game\" function parameters.'''\n",
    "        dataset = []\n",
    "\n",
    "        while len(dataset) < size:\n",
    "            game = self.generate_game(*args)\n",
    "\n",
    "            winner, state =  -1, game[-1].state\n",
    "\n",
    "            i = 0\n",
    "            while state.can_claim_draw():\n",
    "                state = game[- (1 + i)].state\n",
    "                i += 1\n",
    "\n",
    "            outcome = state.outcome()\n",
    "\n",
    "            score = engine.analyse(state, chess.engine.Limit(time=1))[\"score\"].white()\n",
    "            if score > Cp(self.DRAW_CUTOFF):\n",
    "                winner = chess.WHITE\n",
    "            elif score < Cp(self.DRAW_CUTOFF):\n",
    "                winner = chess.BLACK\n",
    "            else:\n",
    "                winner = None\n",
    "\n",
    "            for nd in game:\n",
    "                val = self.get_value(reinf, args[3], winner, nd)\n",
    "\n",
    "                if args[3] == SearchType.MINIMAX:\n",
    "                    position = bitboards.bitboard_to_cnn_input(bitboards.bitboard(nd.get_node().state)).unsqueeze(0).cuda()\n",
    "                    dataset.append([position, val])\n",
    "\n",
    "                elif args[3] == SearchType.MCTS:\n",
    "                    position = bitboards.bitboard_to_cnn_input(bitboards.bitboard(nd.state)).unsqueeze(0).cuda()\n",
    "                    moves = [move.uci() for move in nd.moves]\n",
    "                    policy = helperfuncs.policy_from_probability([[moves[i], child.actionValue] for i, child in enumerate(nd.childNodes)])\n",
    "                    dataset.append([position, val, policy.cuda()])\n",
    "\n",
    "                elif args[3] == SearchType.CUSTOM:\n",
    "                    position = bitboards.bitboard_to_cnn_input(bitboards.bitboard(nd.state)).unsqueeze(0).cuda()\n",
    "                    moves = [move.uci() for move in nd.moves]\n",
    "                    choiceProbability = nd.choiceProbability\n",
    "                    policy = helperfuncs.policy_from_probability([[moves[i], choiceProbability.value(i)] for i in range(len(choiceProbability.x))])\n",
    "                    dataset.append([position, val, policy.cuda()])\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    def generate_game(self, board, nnet, encoder, search_tree, *args):\n",
    "        '''Generate the chess game given the starting \"board\" position. Args depend on chosen search type. \n",
    "        Three parameters for MINIMAX: depth, lower bound and higher bound of aspiration window.\n",
    "        One parameter for MCTS and CUSTOM: number of rollouts.'''\n",
    "\n",
    "        game, moves = [], 0\n",
    "\n",
    "        while not self.stop_cond(board, moves):\n",
    "\n",
    "            if search_tree == SearchType.MINIMAX:\n",
    "                node = alphabeta.alphabeta(alphabeta.Node(board), args[0], args[1], args[2], nnet, encoder)\n",
    "                board = node.get_node().state\n",
    "\n",
    "            elif search_tree == SearchType.MCTS:\n",
    "                tree = mctsAZ.Mcts(board, nnet, encoder)\n",
    "                node = tree.search(args[0])\n",
    "                board = node.state\n",
    "\n",
    "            elif search_tree == SearchType.CUSTOM:\n",
    "                tree = mcts_custom.Mcts(board, nnet, encoder)\n",
    "                node = tree.search(args[0])\n",
    "                board = node.state\n",
    "\n",
    "            game.append(node)\n",
    "            moves += 1\n",
    "\n",
    "        return game\n",
    "\n",
    "    def stop_cond(self, board, moves):\n",
    "        end = False\n",
    "\n",
    "        if board.is_checkmate() or board.is_stalemate() or board.is_insufficient_material():\n",
    "            end = True\n",
    "        elif moves > self.MAX_MOVES:\n",
    "            end = True\n",
    "\n",
    "        return end\n",
    "\n",
    "    def get_value(self, reinf_type, search_type, winner, node):\n",
    "        if reinf_type == ReinforcementType.MC:\n",
    "            val = winner_to_num[winner]\n",
    "        elif reinf_type == ReinforcementType.TD or reinf_type == ReinforcementType.PARAM:\n",
    "            if search_type == SearchType.MINIMAX:\n",
    "                val = node.get_val()[0]\n",
    "            elif search_type == SearchType.MCTS or search_type == SearchType.CUSTOM:\n",
    "                val = node.actionValue\n",
    "\n",
    "        if reinf_type == ReinforcementType.PARAM:\n",
    "            val = self.PARAM * val + (1 - self.PARAM) * winner_to_num[winner]\n",
    "\n",
    "        return torch.Tensor([val]).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27c4bf61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game result mean:  0.75  Standard deviation:  0.4338478284190647\n",
      "Example policy:  [0.06266715 0.03124175 0.         0.         0.         0.21896613\n",
      " 0.         0.03142585 0.         0.         0.         0.12552312\n",
      " 0.         0.         0.         0.03157091 0.         0.\n",
      " 0.03146862 0.         0.06233281 0.14708534 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.03165531 0.         0.         0.         0.\n",
      " 0.         0.22588219 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "##checking few basic statistics about generated datasets\n",
    "# import statistics\n",
    "\n",
    "# encoder = autoencoder.autoencoder().cuda()\n",
    "# encoder.load_state_dict(torch.load(\"autoencoderftest2.pt\"))\n",
    "# nnet = net.Net().cuda()\n",
    "# nnet.load_state_dict(torch.load(\"nnet_mcts.pt\"))\n",
    "\n",
    "# args = (chess.Board(), nnet, encoder, SearchType.CUSTOM, 5)\n",
    "# dataset = SearchDataset(256, Encode(encoder), ReinforcementType.MC, *args)\n",
    "\n",
    "# vals, policies, positions = [], [], []\n",
    "# for position, val, policy in dataset:\n",
    "#     vals.append(val.item())\n",
    "#     policies.append(policy)\n",
    "#     positions.append(position)\n",
    "    \n",
    "# print(\"Game result mean: \", statistics.mean(vals), \" Standard deviation: \", statistics.stdev(vals))\n",
    "\n",
    "# print(\"Example policy: \", policies[32][0].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22eebc7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

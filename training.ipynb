{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e7036a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "import chess\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import dset\n",
    "import net\n",
    "import autoencoder\n",
    "import bitboards\n",
    "\n",
    "c_const = 2\n",
    "samplingRate = 0.4\n",
    "seed = 42\n",
    "mse = nn.MSELoss()\n",
    "\n",
    "def cross_entropy(y_hat, y):\n",
    "    y1, y2 = y[0], y[1]\n",
    "    y_hat1 = (torch.clamp(y_hat[0], 1e-9, 1 - 1e-9))\n",
    "    y_hat2 = (torch.clamp(y_hat[1], 1e-9, 1 - 1e-9))\n",
    "    \n",
    "    return -1/2 * ((y1 * torch.log(y_hat1)).sum(dim=1).mean() + (y2 * torch.log(y_hat2)).sum(dim=1).mean())\n",
    "\n",
    "def train_mcts(batch, dataset_size, encoder, nnet, optimizer, reinf, *args):\n",
    "    dataset = dset.SearchDataset(dataset_size, dset.Encode(encoder), reinf, *args)\n",
    "    pick = math.floor(samplingRate*len(dataset))\n",
    "    subset = torch.utils.data.random_split(dataset, [pick, len(dataset) - pick], generator=torch.Generator().manual_seed(seed))\n",
    "    \n",
    "    DataLoader = torch.utils.data.DataLoader(subset[0], batch_size=batch, shuffle=True, drop_last=True)\n",
    "    \n",
    "    noBatch = 0\n",
    "    for embedding, value, policy in DataLoader:\n",
    "        value_hat, policy_hat = nnet(embedding.view(embedding.shape[0],1, 256))\n",
    "\n",
    "        mse_value = mse(value_hat, value)\n",
    "        cross_entropy_value = cross_entropy(policy_hat, policy)\n",
    "        loss = c_const * mse_value + cross_entropy_value\n",
    "        print(f\"Loss ({noBatch}): \", loss, mse_value, cross_entropy_value, end='\\n')\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        noBatch += 1\n",
    "        \n",
    "    torch.save(nnet.state_dict(), \"nnet_mcts.pt\")\n",
    "                \n",
    "def train_alpha_beta(batch, dataset_size, encoder, nnet, optimizer, reinf, *args):\n",
    "    dataset = dset.SearchDataset(dataset_size, dset.Encode(encoder), reinf, *args)\n",
    "    pick = math.floor(samplingRate*len(dataset))\n",
    "    subset = torch.utils.data.random_split(dataset, [pick, len(dataset) - pick], generator=torch.Generator().manual_seed(seed))\n",
    "    \n",
    "    DataLoader = torch.utils.data.DataLoader(subset[0], batch_size=batch, shuffle=True, drop_last=True)\n",
    "    \n",
    "    noBatch = 0\n",
    "    for embedding, value in DataLoader:\n",
    "        value_hat = nnet(embedding.view(embedding.shape[0],1, 256))\n",
    "\n",
    "        mse_value = mse(value_hat, value)\n",
    "        print(f\"Loss ({noBatch}): \", mse_value, end='\\n')\n",
    "\n",
    "        mse_loss.backward()\n",
    "        optimizer.step()\n",
    "        noBatch += 1\n",
    "        \n",
    "    torch.save(nnet.state_dict(), \"nnet_alpha_beta.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4023221e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mputo/.local/lib/python3.8/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10, 1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss (0):  tensor(0.6215, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2460, device='cuda:0', grad_fn=<MseLossBackward>) tensor(0.1295, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Loss (1):  tensor(0.5593, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.2147, device='cuda:0', grad_fn=<MseLossBackward>) tensor(0.1299, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Loss (2):  tensor(0.4795, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.1750, device='cuda:0', grad_fn=<MseLossBackward>) tensor(0.1295, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Loss (3):  tensor(0.3255, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0984, device='cuda:0', grad_fn=<MseLossBackward>) tensor(0.1287, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Loss (4):  tensor(0.1561, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0152, device='cuda:0', grad_fn=<MseLossBackward>) tensor(0.1256, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Loss (5):  tensor(0.1253, device='cuda:0', grad_fn=<AddBackward0>) tensor(8.9362e-05, device='cuda:0', grad_fn=<MseLossBackward>) tensor(0.1251, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Loss (6):  tensor(0.1130, device='cuda:0', grad_fn=<AddBackward0>) tensor(1.8275e-09, device='cuda:0', grad_fn=<MseLossBackward>) tensor(0.1130, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Loss (7):  tensor(0.1164, device='cuda:0', grad_fn=<AddBackward0>) tensor(0., device='cuda:0', grad_fn=<MseLossBackward>) tensor(0.1164, device='cuda:0', grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "BATCH = 10\n",
    "DATASET_SIZE = 200\n",
    "ARGS = (chess.Board(), net.Net().cuda(), autoencoder.autoencoder().cuda(), dset.SearchType.MCTS, 50)\n",
    "encoder = autoencoder.autoencoder().cuda()\n",
    "nnet = net.Net().cuda()\n",
    "optimizer = optim.Adam(nnet.parameters(), weight_decay=0.01)\n",
    "\n",
    "train_mcts(BATCH, DATASET_SIZE, encoder, nnet, optimizer, dset.ReinforcementType.MC, *ARGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d5795c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
